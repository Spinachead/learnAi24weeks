# pytorch深度学习实践


## 循环神经网络
传统的神经网络（如全连接网络、CNN）假设输入之间是相互独立的。但在很多实际任务中，数据是有前后依赖关系的，比如：
一句话中的词有顺序；
股票价格随时间变化；
视频帧之间存在时间连续性。
RNN 的核心思想是：引入“记忆”机制，让网络在处理当前输入时，能够参考之前的信息

## RNN的典型结构
h_t = f(W_hh * h_{t-1} + W_xh * x_t + b_h)
y_t = g(W_hy * h_t + b_y)


## RNN主要的应用场景
1. 自然语言处理NLP 语言建模 机器翻译 文本生成
2. 语音识别 将音频序列转为文字
3. 时间序列预测：如股票价格 天气预测
4. 视频分析 动作识别 帧间关联


## RNN的局限性
1. 梯度消失问题（vanishing/exploding gradients) 在训练长序列时，误差反向传播过程中梯度会指数级衰减或增长，导致难以学习长期依赖
2. 难以捕捉长距离依赖  即使没有梯度问题，标准 RNN 也很难记住几十步甚至上百步之前的输入信息

## 目前解决RNN局限性的方式
1.LSTM(long Short-Term Memory) 通过“门控机制”（输入门、遗忘门、输出门）控制信息流动，能有效学习长期依赖
2. GRU (Gated Recurrent Unit) LSTM 的简化版，只有两个门（更新门和重置门），计算更高效，性能接近 LSTM。

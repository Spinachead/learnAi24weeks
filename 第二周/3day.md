# B站课程 pytorch深度学习实践

## 线性模型
1. 训练时对数据集采取的策略是一部分作为训练集一部分作为开发集 这样来保证模型的泛化能力
2. 模型训练时先随机选取一个权重 然后通过损失函数计算选取权重和真实数据之间的误差 来获取最佳的权重 误差为零的权重是最好的权重（但真实环境很难有）
3. MSE (均方误差）
4. visdom绘制和查看模型训练时的图
5. 绘制3维图 可能需要使用np.meshgrid()
6. 掌握了np.meshgrid()和np.zeros_like()的用法
7. np.meshgrid() 假设W = [1, 2, 3]，B = [10, 20] meshgrid(W, B)会创建两个矩阵 w_mesh=[[1,2,3],[1,2,3]] b_mesh = [[10,10,10],[20,20,20]]


### 作业 训练出来y=x*w+b 的最佳权重


## 梯度下降算法
> 线性模型采用的是穷举法求取最佳权重w，但是如果一个函数的未知权重有很多的话这种方法就很难算出来了，此时就需要采用梯度下降的算法

- 梯度下降是求取的局部最优权重，但是有可能会陷入鞍点
- 鞍点 一直梯度为0的点
- 训练时如果梯度没有趋近于最小值，反而升高了，此时就是训练发散了，训练发散通常的原因是 学习率取的太大

  ### 随机梯度下降算法
  > 为了解决梯度下降算法的鞍点问题 采用随机梯度下降法算，随机梯度采用的是一个随机样本求梯度
  > 随机梯度下降算法的缺点是无法并行运算 训练耗费的实践比较高
  > 随机梯度下降 虽然性能比较高，但是相应的时间复杂度也太高，梯度下降算法 性能比较低 但是时间复杂度也比较低。通常训练采用折中的办法。批量的随机梯度下降 batch

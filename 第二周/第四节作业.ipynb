{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6bb3cd0-d554-4214-b048-753320aab6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (before training) 4 tensor([21.], grad_fn=<AddBackward0>)\n",
      "\tgrad: 1.0 2.0 2.0 2.0 2.0\n",
      "\tgrad: 2.0 4.0 22.880001068115234 11.440000534057617 5.720000267028809\n",
      "\tgrad: 3.0 6.0 77.04720306396484 25.682401657104492 8.560800552368164\n",
      "Epoch: 0 Loss: 9.167142550150553\n",
      "\tgrad: 1.0 2.0 -1.1466078758239746 -1.1466078758239746 -1.1466078758239746\n",
      "\tgrad: 2.0 4.0 -15.536651611328125 -7.7683258056640625 -3.8841629028320312\n",
      "\tgrad: 3.0 6.0 -30.432214736938477 -10.144071578979492 -3.381357192993164\n",
      "Epoch: 1 Loss: 2.3195839722951255\n",
      "\tgrad: 1.0 2.0 0.3451242446899414 0.3451242446899414 0.3451242446899414\n",
      "\tgrad: 2.0 4.0 2.4273414611816406 1.2136707305908203 0.6068353652954102\n",
      "\tgrad: 3.0 6.0 19.449920654296875 6.483306884765625 2.161102294921875\n",
      "Epoch: 2 Loss: 0.4298102365185817\n",
      "\tgrad: 1.0 2.0 -0.32242679595947266 -0.32242679595947266 -0.32242679595947266\n",
      "\tgrad: 2.0 4.0 -5.845773696899414 -2.922886848449707 -1.4614434242248535\n",
      "\tgrad: 3.0 6.0 -3.8828859329223633 -1.294295310974121 -0.43143177032470703\n",
      "Epoch: 3 Loss: 0.20215910176436105\n",
      "\tgrad: 1.0 2.0 0.01369333267211914 0.01369333267211914 0.01369333267211914\n",
      "\tgrad: 2.0 4.0 -1.9140911102294922 -0.9570455551147461 -0.47852277755737305\n",
      "\tgrad: 3.0 6.0 6.855700492858887 2.285233497619629 0.761744499206543\n",
      "Epoch: 4 Loss: 0.06745218536040436\n",
      "\tgrad: 1.0 2.0 -0.11818885803222656 -0.11818885803222656 -0.11818885803222656\n",
      "\tgrad: 2.0 4.0 -3.664388656616211 -1.8321943283081055 -0.9160971641540527\n",
      "\tgrad: 3.0 6.0 1.7454700469970703 0.5818233489990234 0.1939411163330078\n",
      "Epoch: 5 Loss: 0.07423464651219547\n",
      "\tgrad: 1.0 2.0 -0.03326845169067383 -0.03326845169067383 -0.03326845169067383\n",
      "\tgrad: 2.0 4.0 -2.7738723754882812 -1.3869361877441406 -0.6934680938720703\n",
      "\tgrad: 3.0 6.0 4.014009475708008 1.338003158569336 0.4460010528564453\n",
      "Epoch: 6 Loss: 0.05674347705401791\n",
      "\tgrad: 1.0 2.0 -0.050147056579589844 -0.050147056579589844 -0.050147056579589844\n",
      "\tgrad: 2.0 4.0 -3.1150074005126953 -1.5575037002563477 -0.7787518501281738\n",
      "\tgrad: 3.0 6.0 2.8533897399902344 0.9511299133300781 0.3170433044433594\n",
      "Epoch: 7 Loss: 0.05912380104806895\n",
      "\tgrad: 1.0 2.0 -0.020544052124023438 -0.020544052124023438 -0.020544052124023438\n",
      "\tgrad: 2.0 4.0 -2.8858280181884766 -1.4429140090942383 -0.7214570045471191\n",
      "\tgrad: 3.0 6.0 3.292379379272461 1.0974597930908203 0.36581993103027344\n",
      "Epoch: 8 Loss: 0.054562205868326906\n",
      "\tgrad: 1.0 2.0 -0.013420581817626953 -0.013420581817626953 -0.013420581817626953\n",
      "\tgrad: 2.0 4.0 -2.9246826171875 -1.46234130859375 -0.731170654296875\n",
      "\tgrad: 3.0 6.0 2.990907669067383 0.9969692230224609 0.3323230743408203\n",
      "Epoch: 9 Loss: 0.05376910370250698\n",
      "\tgrad: 1.0 2.0 0.0033445358276367188 0.0033445358276367188 0.0033445358276367188\n",
      "\tgrad: 2.0 4.0 -2.841381072998047 -1.4206905364990234 -0.7103452682495117\n",
      "\tgrad: 3.0 6.0 3.0377025604248047 1.0125675201416016 0.3375225067138672\n",
      "Epoch: 10 Loss: 0.05154358493086875\n",
      "\tgrad: 1.0 2.0 0.014836311340332031 0.014836311340332031 0.014836311340332031\n",
      "\tgrad: 2.0 4.0 -2.8173885345458984 -1.4086942672729492 -0.7043471336364746\n",
      "\tgrad: 3.0 6.0 2.9260196685791016 0.9753398895263672 0.32511329650878906\n",
      "Epoch: 11 Loss: 0.050168638731217165\n",
      "\tgrad: 1.0 2.0 0.028025150299072266 0.028025150299072266 0.028025150299072266\n",
      "\tgrad: 2.0 4.0 -2.768169403076172 -1.384084701538086 -0.692042350769043\n",
      "\tgrad: 3.0 6.0 2.891498565673828 0.9638328552246094 0.3212776184082031\n",
      "Epoch: 12 Loss: 0.04857727687825294\n",
      "\tgrad: 1.0 2.0 0.03969764709472656 0.03969764709472656 0.03969764709472656\n",
      "\tgrad: 2.0 4.0 -2.732961654663086 -1.366480827331543 -0.6832404136657715\n",
      "\tgrad: 3.0 6.0 2.8243446350097656 0.9414482116699219 0.3138160705566406\n",
      "Epoch: 13 Loss: 0.04723949118245704\n",
      "\tgrad: 1.0 2.0 0.051377296447753906 0.051377296447753906 0.051377296447753906\n",
      "\tgrad: 2.0 4.0 -2.6934165954589844 -1.3467082977294922 -0.6733541488647461\n",
      "\tgrad: 3.0 6.0 2.7755842208862305 0.9251947402954102 0.3083982467651367\n",
      "Epoch: 14 Loss: 0.045929575144934155\n",
      "\tgrad: 1.0 2.0 0.062380313873291016 0.062380313873291016 0.062380313873291016\n",
      "\tgrad: 2.0 4.0 -2.6580047607421875 -1.3290023803710938 -0.6645011901855469\n",
      "\tgrad: 3.0 6.0 2.7212963104248047 0.9070987701416016 0.3023662567138672\n",
      "Epoch: 15 Loss: 0.044739872799254954\n",
      "\tgrad: 1.0 2.0 0.07305240631103516 0.07305240631103516 0.07305240631103516\n",
      "\tgrad: 2.0 4.0 -2.622692108154297 -1.3113460540771484 -0.6556730270385742\n",
      "\tgrad: 3.0 6.0 2.672501564025879 0.890833854675293 0.29694461822509766\n",
      "Epoch: 16 Loss: 0.04361832300977161\n",
      "\tgrad: 1.0 2.0 0.08325767517089844 0.08325767517089844 0.08325767517089844\n",
      "\tgrad: 2.0 4.0 -2.589275360107422 -1.294637680053711 -0.6473188400268555\n",
      "\tgrad: 3.0 6.0 2.6239728927612305 0.8746576309204102 0.2915525436401367\n",
      "Epoch: 17 Loss: 0.04257969923006991\n",
      "\tgrad: 1.0 2.0 0.09308338165283203 0.09308338165283203 0.09308338165283203\n",
      "\tgrad: 2.0 4.0 -2.5568485260009766 -1.2784242630004883 -0.6392121315002441\n",
      "\tgrad: 3.0 6.0 2.578036308288574 0.8593454360961914 0.28644847869873047\n",
      "Epoch: 18 Loss: 0.041609115044896804\n",
      "\tgrad: 1.0 2.0 0.10251188278198242 0.10251188278198242 0.10251188278198242\n",
      "\tgrad: 2.0 4.0 -2.5257606506347656 -1.2628803253173828 -0.6314401626586914\n",
      "\tgrad: 3.0 6.0 2.5334815979003906 0.8444938659667969 0.2814979553222656\n",
      "Epoch: 19 Loss: 0.04070553928613663\n",
      "\tgrad: 1.0 2.0 0.1115732192993164 0.1115732192993164 0.1115732192993164\n",
      "\tgrad: 2.0 4.0 -2.4957752227783203 -1.2478876113891602 -0.6239438056945801\n",
      "\tgrad: 3.0 6.0 2.490780830383301 0.8302602767944336 0.27675342559814453\n",
      "Epoch: 20 Loss: 0.039862241984034576\n",
      "\tgrad: 1.0 2.0 0.12027454376220703 0.12027454376220703 0.12027454376220703\n",
      "\tgrad: 2.0 4.0 -2.4669342041015625 -1.2334671020507812 -0.6167335510253906\n",
      "\tgrad: 3.0 6.0 2.4496335983276367 0.8165445327758789 0.27218151092529297\n",
      "Epoch: 21 Loss: 0.03907575155608356\n",
      "\tgrad: 1.0 2.0 0.12863397598266602 0.12863397598266602 0.12863397598266602\n",
      "\tgrad: 2.0 4.0 -2.4391613006591797 -1.2195806503295898 -0.6097903251647949\n",
      "\tgrad: 3.0 6.0 2.4100828170776367 0.8033609390258789 0.26778697967529297\n",
      "Epoch: 22 Loss: 0.0383417330061396\n",
      "\tgrad: 1.0 2.0 0.13666200637817383 0.13666200637817383 0.13666200637817383\n",
      "\tgrad: 2.0 4.0 -2.4124298095703125 -1.2062149047851562 -0.6031074523925781\n",
      "\tgrad: 3.0 6.0 2.3719911575317383 0.7906637191772461 0.26355457305908203\n",
      "Epoch: 23 Loss: 0.03765634261071682\n",
      "\tgrad: 1.0 2.0 0.14437294006347656 0.14437294006347656 0.14437294006347656\n",
      "\tgrad: 2.0 4.0 -2.386688232421875 -1.1933441162109375 -0.5966720581054688\n",
      "\tgrad: 3.0 6.0 2.335367202758789 0.7784557342529297 0.25948524475097656\n",
      "Epoch: 24 Loss: 0.037016141694039106\n",
      "\tgrad: 1.0 2.0 0.1517786979675293 0.1517786979675293 0.1517786979675293\n",
      "\tgrad: 2.0 4.0 -2.3619022369384766 -1.1809511184692383 -0.5904755592346191\n",
      "\tgrad: 3.0 6.0 2.30013370513916 0.7667112350463867 0.2555704116821289\n",
      "Epoch: 25 Loss: 0.036417866901805006\n",
      "\tgrad: 1.0 2.0 0.1588902473449707 0.1588902473449707 0.1588902473449707\n",
      "\tgrad: 2.0 4.0 -2.338043212890625 -1.1690216064453125 -0.5845108032226562\n",
      "\tgrad: 3.0 6.0 2.2661962509155273 0.7553987503051758 0.2517995834350586\n",
      "Epoch: 26 Loss: 0.03585850105931362\n",
      "\tgrad: 1.0 2.0 0.16572046279907227 0.16572046279907227 0.16572046279907227\n",
      "\tgrad: 2.0 4.0 -2.3150596618652344 -1.1575298309326172 -0.5787649154663086\n",
      "\tgrad: 3.0 6.0 2.233572006225586 0.7445240020751953 0.24817466735839844\n",
      "Epoch: 27 Loss: 0.03533523095150789\n",
      "\tgrad: 1.0 2.0 0.17227888107299805 0.17227888107299805 0.17227888107299805\n",
      "\tgrad: 2.0 4.0 -2.2929306030273438 -1.1464653015136719 -0.5732326507568359\n",
      "\tgrad: 3.0 6.0 2.202157974243164 0.7340526580810547 0.24468421936035156\n",
      "Epoch: 28 Loss: 0.034845503978431225\n",
      "\tgrad: 1.0 2.0 0.17857694625854492 0.17857694625854492 0.17857694625854492\n",
      "\tgrad: 2.0 4.0 -2.271617889404297 -1.1358089447021484 -0.5679044723510742\n",
      "\tgrad: 3.0 6.0 2.171945571899414 0.7239818572998047 0.24132728576660156\n",
      "Epoch: 29 Loss: 0.03438700611392657\n",
      "\tgrad: 1.0 2.0 0.18462371826171875 0.18462371826171875 0.18462371826171875\n",
      "\tgrad: 2.0 4.0 -2.2510948181152344 -1.1255474090576172 -0.5627737045288086\n",
      "\tgrad: 3.0 6.0 2.142857551574707 0.7142858505249023 0.23809528350830078\n",
      "Epoch: 30 Loss: 0.033957461516062416\n",
      "\tgrad: 1.0 2.0 0.1904296875 0.1904296875 0.1904296875\n",
      "\tgrad: 2.0 4.0 -2.231321334838867 -1.1156606674194336 -0.5578303337097168\n",
      "\tgrad: 3.0 6.0 2.1148509979248047 0.7049503326416016 0.2349834442138672\n",
      "Epoch: 31 Loss: 0.03355461452156305\n",
      "\tgrad: 1.0 2.0 0.19600486755371094 0.19600486755371094 0.19600486755371094\n",
      "\tgrad: 2.0 4.0 -2.2122726440429688 -1.1061363220214844 -0.5530681610107422\n",
      "\tgrad: 3.0 6.0 2.087925910949707 0.6959753036499023 0.23199176788330078\n",
      "Epoch: 32 Loss: 0.03317687350014845\n",
      "\tgrad: 1.0 2.0 0.2013559341430664 0.2013559341430664 0.2013559341430664\n",
      "\tgrad: 2.0 4.0 -2.193929672241211 -1.0969648361206055 -0.5484824180603027\n",
      "\tgrad: 3.0 6.0 2.061979293823242 0.6873264312744141 0.2291088104248047\n",
      "Epoch: 33 Loss: 0.032822334518035255\n",
      "\tgrad: 1.0 2.0 0.20649433135986328 0.20649433135986328 0.20649433135986328\n",
      "\tgrad: 2.0 4.0 -2.176250457763672 -1.088125228881836 -0.544062614440918\n",
      "\tgrad: 3.0 6.0 2.037019729614258 0.6790065765380859 0.2263355255126953\n",
      "Epoch: 34 Loss: 0.032489316227535404\n",
      "\tgrad: 1.0 2.0 0.21142578125 0.21142578125 0.21142578125\n",
      "\tgrad: 2.0 4.0 -2.1592159271240234 -1.0796079635620117 -0.5398039817810059\n",
      "\tgrad: 3.0 6.0 2.0130043029785156 0.6710014343261719 0.22366714477539062\n",
      "Epoch: 35 Loss: 0.03217634869118532\n",
      "\tgrad: 1.0 2.0 0.21615982055664062 0.21615982055664062 0.21615982055664062\n",
      "\tgrad: 2.0 4.0 -2.142810821533203 -1.0714054107666016 -0.5357027053833008\n",
      "\tgrad: 3.0 6.0 1.9898557662963867 0.6632852554321289 0.22109508514404297\n",
      "Epoch: 36 Loss: 0.0318821237112085\n",
      "\tgrad: 1.0 2.0 0.2207036018371582 0.2207036018371582 0.2207036018371582\n",
      "\tgrad: 2.0 4.0 -2.1269912719726562 -1.0634956359863281 -0.5317478179931641\n",
      "\tgrad: 3.0 6.0 1.967599868774414 0.6558666229248047 0.21862220764160156\n",
      "Epoch: 37 Loss: 0.0316051235422492\n",
      "\tgrad: 1.0 2.0 0.22506427764892578 0.22506427764892578 0.22506427764892578\n",
      "\tgrad: 2.0 4.0 -2.1117515563964844 -1.0558757781982422 -0.5279378890991211\n",
      "\tgrad: 3.0 6.0 1.9461593627929688 0.6487197875976562 0.21623992919921875\n",
      "Epoch: 38 Loss: 0.0313443373888731\n",
      "\tgrad: 1.0 2.0 0.2292490005493164 0.2292490005493164 0.2292490005493164\n",
      "\tgrad: 2.0 4.0 -2.0970611572265625 -1.0485305786132812 -0.5242652893066406\n",
      "\tgrad: 3.0 6.0 1.9255084991455078 0.6418361663818359 0.2139453887939453\n",
      "Epoch: 39 Loss: 0.031098485613862675\n",
      "\tgrad: 1.0 2.0 0.23326539993286133 0.23326539993286133 0.23326539993286133\n",
      "\tgrad: 2.0 4.0 -2.082897186279297 -1.0414485931396484 -0.5207242965698242\n",
      "\tgrad: 3.0 6.0 1.9056644439697266 0.6352214813232422 0.21174049377441406\n",
      "Epoch: 40 Loss: 0.03086671605706215\n",
      "\tgrad: 1.0 2.0 0.23711872100830078 0.23711872100830078 0.23711872100830078\n",
      "\tgrad: 2.0 4.0 -2.0692520141601562 -1.0346260070800781 -0.5173130035400391\n",
      "\tgrad: 3.0 6.0 1.8864898681640625 0.6288299560546875 0.2096099853515625\n",
      "Epoch: 41 Loss: 0.03064786580701669\n",
      "\tgrad: 1.0 2.0 0.24081659317016602 0.24081659317016602 0.24081659317016602\n",
      "\tgrad: 2.0 4.0 -2.0560836791992188 -1.0280418395996094 -0.5140209197998047\n",
      "\tgrad: 3.0 6.0 1.8680963516235352 0.6226987838745117 0.2075662612915039\n",
      "Epoch: 42 Loss: 0.030441156588494778\n",
      "\tgrad: 1.0 2.0 0.24436283111572266 0.24436283111572266 0.24436283111572266\n",
      "\tgrad: 2.0 4.0 -2.0433998107910156 -1.0216999053955078 -0.5108499526977539\n",
      "\tgrad: 3.0 6.0 1.850320816040039 0.6167736053466797 0.20559120178222656\n",
      "Epoch: 43 Loss: 0.030245717304448288\n",
      "\tgrad: 1.0 2.0 0.24776697158813477 0.24776697158813477 0.24776697158813477\n",
      "\tgrad: 2.0 4.0 -2.0311546325683594 -1.0155773162841797 -0.5077886581420898\n",
      "\tgrad: 3.0 6.0 1.8332405090332031 0.6110801696777344 0.20369338989257812\n",
      "Epoch: 44 Loss: 0.030060733978947003\n",
      "\tgrad: 1.0 2.0 0.25103092193603516 0.25103092193603516 0.25103092193603516\n",
      "\tgrad: 2.0 4.0 -2.0193519592285156 -1.0096759796142578 -0.5048379898071289\n",
      "\tgrad: 3.0 6.0 1.816786766052246 0.605595588684082 0.20186519622802734\n",
      "Epoch: 45 Loss: 0.029885622362295788\n",
      "\tgrad: 1.0 2.0 0.2541618347167969 0.2541618347167969 0.2541618347167969\n",
      "\tgrad: 2.0 4.0 -2.0079689025878906 -1.0039844512939453 -0.5019922256469727\n",
      "\tgrad: 3.0 6.0 1.8009252548217773 0.6003084182739258 0.2001028060913086\n",
      "Epoch: 46 Loss: 0.029719630256295204\n",
      "\tgrad: 1.0 2.0 0.25716400146484375 0.25716400146484375 0.25716400146484375\n",
      "\tgrad: 2.0 4.0 -1.9969863891601562 -0.9984931945800781 -0.49924659729003906\n",
      "\tgrad: 3.0 6.0 1.785630226135254 0.595210075378418 0.19840335845947266\n",
      "Epoch: 47 Loss: 0.029562031229337055\n",
      "\tgrad: 1.0 2.0 0.2600440979003906 0.2600440979003906 0.2600440979003906\n",
      "\tgrad: 2.0 4.0 -1.9863853454589844 -0.9931926727294922 -0.4965963363647461\n",
      "\tgrad: 3.0 6.0 1.7709360122680664 0.5903120040893555 0.19677066802978516\n",
      "Epoch: 48 Loss: 0.029412461755176384\n",
      "\tgrad: 1.0 2.0 0.2628040313720703 0.2628040313720703 0.2628040313720703\n",
      "\tgrad: 2.0 4.0 -1.976165771484375 -0.9880828857421875 -0.49404144287109375\n",
      "\tgrad: 3.0 6.0 1.7567567825317383 0.5855855941772461 0.19519519805908203\n",
      "Epoch: 49 Loss: 0.029270339757204056\n",
      "\tgrad: 1.0 2.0 0.26545143127441406 0.26545143127441406 0.26545143127441406\n",
      "\tgrad: 2.0 4.0 -1.9663009643554688 -0.9831504821777344 -0.4915752410888672\n",
      "\tgrad: 3.0 6.0 1.7430925369262695 0.5810308456420898 0.19367694854736328\n",
      "Epoch: 50 Loss: 0.029135119790832203\n",
      "\tgrad: 1.0 2.0 0.2679886817932129 0.2679886817932129 0.2679886817932129\n",
      "\tgrad: 2.0 4.0 -1.9567756652832031 -0.9783878326416016 -0.4891939163208008\n",
      "\tgrad: 3.0 6.0 1.7299346923828125 0.5766448974609375 0.1922149658203125\n",
      "Epoch: 51 Loss: 0.029006267587343853\n",
      "\tgrad: 1.0 2.0 0.27042055130004883 0.27042055130004883 0.27042055130004883\n",
      "\tgrad: 2.0 4.0 -1.9475860595703125 -0.9737930297851562 -0.4868965148925781\n",
      "\tgrad: 3.0 6.0 1.717240333557129 0.572413444519043 0.19080448150634766\n",
      "Epoch: 52 Loss: 0.028883487296601135\n",
      "\tgrad: 1.0 2.0 0.2727518081665039 0.2727518081665039 0.2727518081665039\n",
      "\tgrad: 2.0 4.0 -1.9387092590332031 -0.9693546295166016 -0.4846773147583008\n",
      "\tgrad: 3.0 6.0 1.705026626586914 0.5683422088623047 0.18944740295410156\n",
      "Epoch: 53 Loss: 0.02876633033156395\n",
      "\tgrad: 1.0 2.0 0.27498531341552734 0.27498531341552734 0.27498531341552734\n",
      "\tgrad: 2.0 4.0 -1.9301414489746094 -0.9650707244873047 -0.48253536224365234\n",
      "\tgrad: 3.0 6.0 1.6932334899902344 0.5644111633300781 0.18813705444335938\n",
      "Epoch: 54 Loss: 0.02865440429498752\n",
      "\tgrad: 1.0 2.0 0.27712535858154297 0.27712535858154297 0.27712535858154297\n",
      "\tgrad: 2.0 4.0 -1.9218635559082031 -0.9609317779541016 -0.4804658889770508\n",
      "\tgrad: 3.0 6.0 1.6818780899047852 0.5606260299682617 0.1868753433227539\n",
      "Epoch: 55 Loss: 0.0285473611826698\n",
      "\tgrad: 1.0 2.0 0.2791757583618164 0.2791757583618164 0.2791757583618164\n",
      "\tgrad: 2.0 4.0 -1.9138717651367188 -0.9569358825683594 -0.4784679412841797\n",
      "\tgrad: 3.0 6.0 1.6709346771240234 0.5569782257080078 0.18565940856933594\n",
      "Epoch: 56 Loss: 0.028445007589956123\n",
      "\tgrad: 1.0 2.0 0.2811393737792969 0.2811393737792969 0.2811393737792969\n",
      "\tgrad: 2.0 4.0 -1.9061508178710938 -0.9530754089355469 -0.47653770446777344\n",
      "\tgrad: 3.0 6.0 1.6603689193725586 0.5534563064575195 0.18448543548583984\n",
      "Epoch: 57 Loss: 0.02834686730057001\n",
      "\tgrad: 1.0 2.0 0.28302001953125 0.28302001953125 0.28302001953125\n",
      "\tgrad: 2.0 4.0 -1.8986892700195312 -0.9493446350097656 -0.4746723175048828\n",
      "\tgrad: 3.0 6.0 1.6501893997192383 0.5500631332397461 0.18335437774658203\n",
      "Epoch: 58 Loss: 0.028252747220297653\n",
      "\tgrad: 1.0 2.0 0.284820556640625 0.284820556640625 0.284820556640625\n",
      "\tgrad: 2.0 4.0 -1.8914775848388672 -0.9457387924194336 -0.4728693962097168\n",
      "\tgrad: 3.0 6.0 1.6403875350952148 0.5467958450317383 0.1822652816772461\n",
      "Epoch: 59 Loss: 0.02816240427394708\n",
      "\tgrad: 1.0 2.0 0.2865438461303711 0.2865438461303711 0.2865438461303711\n",
      "\tgrad: 2.0 4.0 -1.8845138549804688 -0.9422569274902344 -0.4711284637451172\n",
      "\tgrad: 3.0 6.0 1.630894660949707 0.5436315536499023 0.18121051788330078\n",
      "Epoch: 60 Loss: 0.028075555029014748\n",
      "\tgrad: 1.0 2.0 0.2881946563720703 0.2881946563720703 0.2881946563720703\n",
      "\tgrad: 2.0 4.0 -1.8777713775634766 -0.9388856887817383 -0.46944284439086914\n",
      "\tgrad: 3.0 6.0 1.621779441833496 0.540593147277832 0.18019771575927734\n",
      "Epoch: 61 Loss: 0.027991996457179386\n",
      "\tgrad: 1.0 2.0 0.28977346420288086 0.28977346420288086 0.28977346420288086\n",
      "\tgrad: 2.0 4.0 -1.8712615966796875 -0.9356307983398438 -0.4678153991699219\n",
      "\tgrad: 3.0 6.0 1.6129646301269531 0.5376548767089844 0.17921829223632812\n",
      "Epoch: 62 Loss: 0.027911592585345108\n",
      "\tgrad: 1.0 2.0 0.29128456115722656 0.29128456115722656 0.29128456115722656\n",
      "\tgrad: 2.0 4.0 -1.8649616241455078 -0.9324808120727539 -0.46624040603637695\n",
      "\tgrad: 3.0 6.0 1.6044673919677734 0.5348224639892578 0.17827415466308594\n",
      "Epoch: 63 Loss: 0.02783404104411602\n",
      "\tgrad: 1.0 2.0 0.29272985458374023 0.29272985458374023 0.29272985458374023\n",
      "\tgrad: 2.0 4.0 -1.8588676452636719 -0.9294338226318359 -0.46471691131591797\n",
      "\tgrad: 3.0 6.0 1.5962448120117188 0.5320816040039062 0.17736053466796875\n",
      "Epoch: 64 Loss: 0.02775911179681619\n",
      "\tgrad: 1.0 2.0 0.2941126823425293 0.2941126823425293 0.2941126823425293\n",
      "\tgrad: 2.0 4.0 -1.8529701232910156 -0.9264850616455078 -0.4632425308227539\n",
      "\tgrad: 3.0 6.0 1.5883655548095703 0.5294551849365234 0.1764850616455078\n",
      "Epoch: 65 Loss: 0.027686907909810543\n",
      "\tgrad: 1.0 2.0 0.29543399810791016 0.29543399810791016 0.29543399810791016\n",
      "\tgrad: 2.0 4.0 -1.8472747802734375 -0.9236373901367188 -0.4618186950683594\n",
      "\tgrad: 3.0 6.0 1.5806922912597656 0.5268974304199219 0.17563247680664062\n",
      "Epoch: 66 Loss: 0.02761704350511233\n",
      "\tgrad: 1.0 2.0 0.29669761657714844 0.29669761657714844 0.29669761657714844\n",
      "\tgrad: 2.0 4.0 -1.841745376586914 -0.920872688293457 -0.4604363441467285\n",
      "\tgrad: 3.0 6.0 1.5733451843261719 0.5244483947753906 0.17481613159179688\n",
      "Epoch: 67 Loss: 0.02754931462307771\n",
      "\tgrad: 1.0 2.0 0.29790449142456055 0.29790449142456055 0.29790449142456055\n",
      "\tgrad: 2.0 4.0 -1.8364067077636719 -0.9182033538818359 -0.45910167694091797\n",
      "\tgrad: 3.0 6.0 1.5662040710449219 0.5220680236816406 0.17402267456054688\n",
      "Epoch: 68 Loss: 0.027483776832620304\n",
      "\tgrad: 1.0 2.0 0.2990589141845703 0.2990589141845703 0.2990589141845703\n",
      "\tgrad: 2.0 4.0 -1.8312263488769531 -0.9156131744384766 -0.4578065872192383\n",
      "\tgrad: 3.0 6.0 1.5593376159667969 0.5197792053222656 0.17325973510742188\n",
      "Epoch: 69 Loss: 0.02742017013952136\n",
      "\tgrad: 1.0 2.0 0.30016040802001953 0.30016040802001953 0.30016040802001953\n",
      "\tgrad: 2.0 4.0 -1.8262138366699219 -0.9131069183349609 -0.45655345916748047\n",
      "\tgrad: 3.0 6.0 1.552694320678711 0.5175647735595703 0.17252159118652344\n",
      "Epoch: 70 Loss: 0.027358419572313625\n",
      "\tgrad: 1.0 2.0 0.30121278762817383 0.30121278762817383 0.30121278762817383\n",
      "\tgrad: 2.0 4.0 -1.8213577270507812 -0.9106788635253906 -0.4553394317626953\n",
      "\tgrad: 3.0 6.0 1.5462827682495117 0.5154275894165039 0.17180919647216797\n",
      "Epoch: 71 Loss: 0.02729846133540074\n",
      "\tgrad: 1.0 2.0 0.3022174835205078 0.3022174835205078 0.3022174835205078\n",
      "\tgrad: 2.0 4.0 -1.8166522979736328 -0.9083261489868164 -0.4541630744934082\n",
      "\tgrad: 3.0 6.0 1.5400772094726562 0.5133590698242188 0.17111968994140625\n",
      "Epoch: 72 Loss: 0.02724012080579996\n",
      "\tgrad: 1.0 2.0 0.3031759262084961 0.3031759262084961 0.3031759262084961\n",
      "\tgrad: 2.0 4.0 -1.8120880126953125 -0.9060440063476562 -0.4530220031738281\n",
      "\tgrad: 3.0 6.0 1.5340948104858398 0.5113649368286133 0.1704549789428711\n",
      "Epoch: 73 Loss: 0.02718329041575392\n",
      "\tgrad: 1.0 2.0 0.3040900230407715 0.3040900230407715 0.3040900230407715\n",
      "\tgrad: 2.0 4.0 -1.8076667785644531 -0.9038333892822266 -0.4519166946411133\n",
      "\tgrad: 3.0 6.0 1.5283098220825195 0.5094366073608398 0.16981220245361328\n",
      "Epoch: 74 Loss: 0.02712796876827876\n",
      "\tgrad: 1.0 2.0 0.304962158203125 0.304962158203125 0.304962158203125\n",
      "\tgrad: 2.0 4.0 -1.8033790588378906 -0.9016895294189453 -0.45084476470947266\n",
      "\tgrad: 3.0 6.0 1.5227222442626953 0.5075740814208984 0.1691913604736328\n",
      "Epoch: 75 Loss: 0.027074053107450407\n",
      "\tgrad: 1.0 2.0 0.30579280853271484 0.30579280853271484 0.30579280853271484\n",
      "\tgrad: 2.0 4.0 -1.7992210388183594 -0.8996105194091797 -0.44980525970458984\n",
      "\tgrad: 3.0 6.0 1.5172977447509766 0.5057659149169922 0.16858863830566406\n",
      "Epoch: 76 Loss: 0.027021345682442188\n",
      "\tgrad: 1.0 2.0 0.30658483505249023 0.30658483505249023 0.30658483505249023\n",
      "\tgrad: 2.0 4.0 -1.7951812744140625 -0.8975906372070312 -0.4487953186035156\n",
      "\tgrad: 3.0 6.0 1.5120878219604492 0.5040292739868164 0.16800975799560547\n",
      "Epoch: 77 Loss: 0.026969898026436567\n",
      "\tgrad: 1.0 2.0 0.3073387145996094 0.3073387145996094 0.3073387145996094\n",
      "\tgrad: 2.0 4.0 -1.791269302368164 -0.895634651184082 -0.447817325592041\n",
      "\tgrad: 3.0 6.0 1.5070152282714844 0.5023384094238281 0.16744613647460938\n",
      "Epoch: 78 Loss: 0.02691963796193401\n",
      "\tgrad: 1.0 2.0 0.3080568313598633 0.3080568313598633 0.3080568313598633\n",
      "\tgrad: 2.0 4.0 -1.7874603271484375 -0.8937301635742188 -0.4468650817871094\n",
      "\tgrad: 3.0 6.0 1.502131462097168 0.5007104873657227 0.16690349578857422\n",
      "Epoch: 79 Loss: 0.026870349577317636\n",
      "\tgrad: 1.0 2.0 0.30873966217041016 0.30873966217041016 0.30873966217041016\n",
      "\tgrad: 2.0 4.0 -1.7837696075439453 -0.8918848037719727 -0.44594240188598633\n",
      "\tgrad: 3.0 6.0 1.4973936080932617 0.4991312026977539 0.16637706756591797\n",
      "Epoch: 80 Loss: 0.026822177693247795\n",
      "\tgrad: 1.0 2.0 0.309389591217041 0.309389591217041 0.309389591217041\n",
      "\tgrad: 2.0 4.0 -1.780181884765625 -0.8900909423828125 -0.44504547119140625\n",
      "\tgrad: 3.0 6.0 1.492818832397461 0.4976062774658203 0.16586875915527344\n",
      "Epoch: 81 Loss: 0.02677498695751031\n",
      "\tgrad: 1.0 2.0 0.31000614166259766 0.31000614166259766 0.31000614166259766\n",
      "\tgrad: 2.0 4.0 -1.7766971588134766 -0.8883485794067383 -0.44417428970336914\n",
      "\tgrad: 3.0 6.0 1.4883899688720703 0.49612998962402344 0.1653766632080078\n",
      "Epoch: 82 Loss: 0.026728670268009107\n",
      "\tgrad: 1.0 2.0 0.3105926513671875 0.3105926513671875 0.3105926513671875\n",
      "\tgrad: 2.0 4.0 -1.7733135223388672 -0.8866567611694336 -0.4433283805847168\n",
      "\tgrad: 3.0 6.0 1.4840812683105469 0.4946937561035156 0.16489791870117188\n",
      "Epoch: 83 Loss: 0.026683264567206304\n",
      "\tgrad: 1.0 2.0 0.31114959716796875 0.31114959716796875 0.31114959716796875\n",
      "\tgrad: 2.0 4.0 -1.7700138092041016 -0.8850069046020508 -0.4425034523010254\n",
      "\tgrad: 3.0 6.0 1.4799528121948242 0.4933176040649414 0.16443920135498047\n",
      "Epoch: 84 Loss: 0.026638635744651157\n",
      "\tgrad: 1.0 2.0 0.3116769790649414 0.3116769790649414 0.3116769790649414\n",
      "\tgrad: 2.0 4.0 -1.7668170928955078 -0.8834085464477539 -0.44170427322387695\n",
      "\tgrad: 3.0 6.0 1.4759016036987305 0.49196720123291016 0.16398906707763672\n",
      "Epoch: 85 Loss: 0.02659480134025216\n",
      "\tgrad: 1.0 2.0 0.3121776580810547 0.3121776580810547 0.3121776580810547\n",
      "\tgrad: 2.0 4.0 -1.7636966705322266 -0.8818483352661133 -0.44092416763305664\n",
      "\tgrad: 3.0 6.0 1.4720134735107422 0.49067115783691406 0.1635570526123047\n",
      "Epoch: 86 Loss: 0.026551660305509966\n",
      "\tgrad: 1.0 2.0 0.3126516342163086 0.3126516342163086 0.3126516342163086\n",
      "\tgrad: 2.0 4.0 -1.7606639862060547 -0.8803319931030273 -0.44016599655151367\n",
      "\tgrad: 3.0 6.0 1.4682197570800781 0.4894065856933594 0.16313552856445312\n",
      "Epoch: 87 Loss: 0.0265091957213978\n",
      "\tgrad: 1.0 2.0 0.31310081481933594 0.31310081481933594 0.31310081481933594\n",
      "\tgrad: 2.0 4.0 -1.7577095031738281 -0.8788547515869141 -0.43942737579345703\n",
      "\tgrad: 3.0 6.0 1.4645805358886719 0.4881935119628906 0.16273117065429688\n",
      "Epoch: 88 Loss: 0.026467497150103252\n",
      "\tgrad: 1.0 2.0 0.3135242462158203 0.3135242462158203 0.3135242462158203\n",
      "\tgrad: 2.0 4.0 -1.754842758178711 -0.8774213790893555 -0.43871068954467773\n",
      "\tgrad: 3.0 6.0 1.4610099792480469 0.4870033264160156 0.16233444213867188\n",
      "Epoch: 89 Loss: 0.026426415735234816\n",
      "\tgrad: 1.0 2.0 0.31392478942871094 0.31392478942871094 0.31392478942871094\n",
      "\tgrad: 2.0 4.0 -1.7520370483398438 -0.8760185241699219 -0.43800926208496094\n",
      "\tgrad: 3.0 6.0 1.457585334777832 0.48586177825927734 0.16195392608642578\n",
      "Epoch: 90 Loss: 0.02638582931831479\n",
      "\tgrad: 1.0 2.0 0.314302921295166 0.314302921295166 0.314302921295166\n",
      "\tgrad: 2.0 4.0 -1.7493114471435547 -0.8746557235717773 -0.43732786178588867\n",
      "\tgrad: 3.0 6.0 1.4542293548583984 0.4847431182861328 0.16158103942871094\n",
      "Epoch: 91 Loss: 0.02634586797406276\n",
      "\tgrad: 1.0 2.0 0.31465959548950195 0.31465959548950195 0.31465959548950195\n",
      "\tgrad: 2.0 4.0 -1.7466468811035156 -0.8733234405517578 -0.4366617202758789\n",
      "\tgrad: 3.0 6.0 1.4509849548339844 0.4836616516113281 0.16122055053710938\n",
      "Epoch: 92 Loss: 0.026306348542372387\n",
      "\tgrad: 1.0 2.0 0.31499528884887695 0.31499528884887695 0.31499528884887695\n",
      "\tgrad: 2.0 4.0 -1.7440509796142578 -0.8720254898071289 -0.43601274490356445\n",
      "\tgrad: 3.0 6.0 1.4478435516357422 0.48261451721191406 0.1608715057373047\n",
      "Epoch: 93 Loss: 0.026267398614436388\n",
      "\tgrad: 1.0 2.0 0.3153109550476074 0.3153109550476074 0.3153109550476074\n",
      "\tgrad: 2.0 4.0 -1.7415199279785156 -0.8707599639892578 -0.4353799819946289\n",
      "\tgrad: 3.0 6.0 1.4447879791259766 0.4815959930419922 0.16053199768066406\n",
      "Epoch: 94 Loss: 0.02622893825173378\n",
      "\tgrad: 1.0 2.0 0.31560707092285156 0.31560707092285156 0.31560707092285156\n",
      "\tgrad: 2.0 4.0 -1.7390518188476562 -0.8695259094238281 -0.43476295471191406\n",
      "\tgrad: 3.0 6.0 1.4418182373046875 0.4806060791015625 0.1602020263671875\n",
      "Epoch: 95 Loss: 0.02619094541296363\n",
      "\tgrad: 1.0 2.0 0.3158855438232422 0.3158855438232422 0.3158855438232422\n",
      "\tgrad: 2.0 4.0 -1.7366409301757812 -0.8683204650878906 -0.4341602325439453\n",
      "\tgrad: 3.0 6.0 1.4389429092407227 0.4796476364135742 0.1598825454711914\n",
      "Epoch: 96 Loss: 0.02615343468884627\n",
      "\tgrad: 1.0 2.0 0.3161449432373047 0.3161449432373047 0.3161449432373047\n",
      "\tgrad: 2.0 4.0 -1.7342891693115234 -0.8671445846557617 -0.43357229232788086\n",
      "\tgrad: 3.0 6.0 1.436136245727539 0.4787120819091797 0.15957069396972656\n",
      "Epoch: 97 Loss: 0.026116279885172844\n",
      "\tgrad: 1.0 2.0 0.3163881301879883 0.3163881301879883 0.3163881301879883\n",
      "\tgrad: 2.0 4.0 -1.7319889068603516 -0.8659944534301758 -0.4329972267150879\n",
      "\tgrad: 3.0 6.0 1.4334239959716797 0.47780799865722656 0.1592693328857422\n",
      "Epoch: 98 Loss: 0.026079564355313778\n",
      "\tgrad: 1.0 2.0 0.31661415100097656 0.31661415100097656 0.31661415100097656\n",
      "\tgrad: 2.0 4.0 -1.7297439575195312 -0.8648719787597656 -0.4324359893798828\n",
      "\tgrad: 3.0 6.0 1.4307546615600586 0.47691822052001953 0.15897274017333984\n",
      "Epoch: 99 Loss: 0.02604314498603344\n",
      "Predict (after training) 4 8.544171333312988\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w1 = torch.tensor([1.0], requires_grad=True)  # 初始权值\n",
    "w2 = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w1 * x**2 + w2 * x + b\n",
    "\n",
    "def loss(x, y):  # 构建计算图\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "print('Predict (before training)', 4, forward(4))\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0  # 用于累计本轮所有样本的损失\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()  # 计算梯度\n",
    "        \n",
    "        print('\\tgrad:', x, y, w1.grad.item(), w2.grad.item(), b.grad.item())\n",
    "        total_loss += l.item()\n",
    "        \n",
    "        # 更新参数 (手动梯度下降)\n",
    "        with torch.no_grad():  # 确保更新操作不会被跟踪\n",
    "            w1 -= 0.01 * w1.grad\n",
    "            w2 -= 0.01 * w2.grad\n",
    "            b -= 0.01 * b.grad\n",
    "            \n",
    "            # 清零梯度\n",
    "            w1.grad.zero_()\n",
    "            w2.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    print('Epoch:', epoch, 'Loss:', total_loss/len(x_data))  # 输出本轮平均损失\n",
    "\n",
    "print('Predict (after training)', 4, forward(4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550ffc6-69ef-4800-b4a8-4f95110c3956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

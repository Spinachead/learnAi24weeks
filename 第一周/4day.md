## 读《深度学习入门：基于python的理论与实现》
> 机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）​。这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。
------
## 神经网络的学习分为以下4个步骤
1. mini-batch 从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值
2. 计算梯度 为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向
3. 更新参数 将权重参数沿梯度方向进行微小更新
4. 重复步骤1，2，3
---

**随机梯度下降法一般由一个名为SGD的函数来实现**
## 误差反向传播法
## 神经网络学习的全面图
> 前提 神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面4个步骤
1. 从训练数据中随机选择一部分数据
2. 计算损失函数关于各个权重参数的梯度
3. 将权重参数沿梯度方向进行微小的更新
4. 重复步骤1、步骤2、步骤3

## 数值微分和对应误差反向传播
- 数值微分法求得梯度比较耗费时间
- 通常使用误差反向传播法求梯度
- 数值微分的优点是实现简单，而误差反向传播法实现复杂。经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认

  ## 随机梯度下降法
  > 为寻找最优参数，我们利用梯度沿其方向迭代更新参数，此过程称为随机梯度下降（SGD）。

## 过拟合
> 产生过拟合的原因主要是一下两个 1：模型拥有大量参数 表现性强；2：训练数据少
---
## 解决过拟合的方法
- 权值衰减 通过在学习的过程中对大的权重进行乘法来抑制过拟合
- Dropout 在学习的过程中随机删除神经元

### 第六章总结
- 为寻找最优参数，我们利用梯度沿其方向迭代更新参数，此过程称为随机梯度下降（SGD）。
- 权重初始值的赋值方法对进行正确的学习非常重要
- 作为权重初始值，Xavier初始值、He初始值等比较有效
- 通过使用Batch Normalization，可以加速学习，并且对初始值变得健壮
- 抑制过拟合的正则化技术有权值衰减、Dropout等
- 
